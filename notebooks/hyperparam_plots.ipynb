{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T17:03:09.754700Z",
     "start_time": "2023-04-15T17:03:09.116738Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import math\n",
    "from ipywidgets import widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T17:03:09.760564Z",
     "start_time": "2023-04-15T17:03:09.759779Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# dataset = 'infectious'\n",
    "# beta = 1\n",
    "# Delta_T = 0.1 # Fraction of dataset to select (this is then divided by phi)\n",
    "# k = 10\n",
    "# phi = 0.1\n",
    "# lines = 0\n",
    "# runs = 3\n",
    "\n",
    "def get_data(dataset, Delta_T, phi):\n",
    "    lines = 0\n",
    "    if dataset == \"infectious\":\n",
    "        lines = 17298\n",
    "    elif dataset == \"ht09_contact\":\n",
    "        lines = 20818\n",
    "    elif dataset == \"SFHH\":\n",
    "        lines = 70261\n",
    "    elif dataset == \"tij\":\n",
    "        lines = 78249\n",
    "\n",
    "    total_edges = int(lines * Delta_T)\n",
    "    random_start_point = np.random.uniform(0, float(1 - Delta_T))\n",
    "    start = int(lines * random_start_point)\n",
    "\n",
    "    train_lines = int(total_edges * float(1 - phi))\n",
    "    mid = start + train_lines\n",
    "    end = start + total_edges\n",
    "\n",
    "    print(f'Total lines: {total_edges}, train lines {train_lines}')\n",
    "    print(f'Starting at {start} till {mid} to predict till {end}')\n",
    "\n",
    "    path_to_file = f'../data/{dataset}.txt'\n",
    "\n",
    "    all_data = np.loadtxt(path_to_file, delimiter='\\t', dtype=int)[start: end]\n",
    "    train_data = all_data[:train_lines]\n",
    "    predict_data = all_data[train_lines:]\n",
    "    new_points = predict_data[-1][2] - predict_data[0][2] + 1\n",
    "    print(f'Starting at {train_data[0]} till {predict_data[0]} to predict till {predict_data[-1]}')\n",
    "\n",
    "\n",
    "    return all_data, train_data, predict_data, train_lines, total_edges, start, mid, end, new_points\n",
    "\n",
    "# Check all edges are unique\n",
    "\n",
    "# a = np.array([src, dst, ts])\n",
    "# a = a.T\n",
    "# # np.any(np.all(np.unique(a, axis=1) == a, axis=1))\n",
    "# u, c = np.unique(a, return_counts=True, axis=0)\n",
    "# u[c > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T17:03:09.786817Z",
     "start_time": "2023-04-15T17:03:09.763111Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construct Graph\n",
    "def make_graph(all_data):\n",
    "    src, dst, ts = [], [], []\n",
    "    graph = nx.MultiGraph()\n",
    "\n",
    "    for line in all_data:\n",
    "        src.append(line[0])\n",
    "        dst.append(line[1])\n",
    "        ts.append(line[2])\n",
    "\n",
    "        if graph.nodes.get(line[0]) is None:\n",
    "            graph.add_node(line[0], is_infected=False, infected_at=math.inf, influence=-1, influences=None, pred=None, polyfit_prediction=None)\n",
    "        if graph.nodes.get(line[1]) is None:\n",
    "            graph.add_node(line[1], is_infected=False, infected_at=math.inf, influence=-1, influences=None, pred=None, polyfit_prediction=None)\n",
    "\n",
    "        graph.add_edge(line[0], line[1], key=line[2], timestamp=line[2])\n",
    "\n",
    "    src, dst, ts = np.array(src), np.array(dst), np.array(ts) # ts is given sorted\n",
    "\n",
    "    return src, dst, ts, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T17:03:09.833858Z",
     "start_time": "2023-04-15T17:03:09.789261Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_holes_timestamps(start, mid, end, influences):\n",
    "    xs, ys = [], []\n",
    "    for i in range(start, end + 1):\n",
    "        if i < mid:\n",
    "            xs.append((i, influences.get(str(i), 0)))\n",
    "        else:\n",
    "            ys.append((i, influences.get(str(i), 0)))\n",
    "            \n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T17:03:09.834014Z",
     "start_time": "2023-04-15T17:03:09.833773Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_edge(edge, g, inf, ins, beta):\n",
    "    if g.nodes.get(edge[0])['is_infected'] and edge[2] > g.nodes.get(edge[0])['infected_at'] and not g.nodes.get(edge[1])['is_infected'] and np.random.random() <= beta:\n",
    "        inf += 1\n",
    "        g.nodes.get(edge[1])['in_infected'] = True\n",
    "        g.nodes.get(edge[1])['infected_at'] = edge[2]\n",
    "        ins[str(edge[2])] = ins.get(str(edge[2]), 0) + 1\n",
    "    \n",
    "    return inf, ins, g\n",
    "\n",
    "def find_influence_of_node(edges, node, at, beta):\n",
    "    g = nx.Graph()\n",
    "    influence, influences, pred = 0, {}, {}\n",
    "    \n",
    "    for edge in edges:\n",
    "        if g.nodes.get(edge[0]) is None:\n",
    "            g.add_node(edge[0], is_infected=False, infected_at=math.inf)\n",
    "        if g.nodes.get(edge[1]) is None:\n",
    "            g.add_node(edge[1], is_infected=False, infected_at=math.inf)\n",
    "\n",
    "    g.nodes.get(node)['is_infected'] = True\n",
    "    g.nodes.get(node)['infected_at'] = at\n",
    "\n",
    "    # TODO: Check influence. I think it's getting unset somewhere because of how Python handles pointers.\n",
    "    for i, edge in enumerate(edges):\n",
    "        influence, influences, g = check_edge(edge, g, influence, influences, beta)\n",
    "        new_edge = (edge[1], edge[0], edge[2])\n",
    "        influence, influences, g = check_edge(new_edge, g, influence, influences, beta)\n",
    "        \n",
    "    return influence, influences\n",
    "\n",
    "# TODO: Fix this\n",
    "def find_influence_in_graph(graph, all_data, beta):\n",
    "    for i, edge in enumerate(all_data):\n",
    "        for e in [edge[0], edge[1]]:\n",
    "            if graph.nodes.get(e)['influence'] == -1:\n",
    "                z, zz = find_influence_of_node(all_data, e, edge[2], beta)\n",
    "                graph.nodes.get(e)['influence'] = z\n",
    "                graph.nodes.get(e)['influences'] = zz\n",
    "\n",
    "    most_influential_nodes = sorted(list(graph.nodes.data(\"influence\")), key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(f\"most influentual nodes: {most_influential_nodes}\")\n",
    "    return most_influential_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T17:03:09.880501Z",
     "start_time": "2023-04-15T17:03:09.833838Z"
    }
   },
   "outputs": [],
   "source": [
    "def polyfit(data, new_points, dimension):\n",
    "    #print(f\"new_points: {new_points}\")\n",
    "    data = np.array(list(map(lambda x: [x[0], x[1]], data)))\n",
    "    fit = np.polyfit(data[:, 0], data[:, 1], 1) # The use of 1 signifies a linear fit.\n",
    "\n",
    "    line = np.poly1d(fit)\n",
    "    new_points = np.arange(new_points) + (data[-1][0] + 1)\n",
    "    return np.cumsum(line(new_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T17:03:09.880655Z",
     "start_time": "2023-04-15T17:03:09.880427Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_1(most_influential_nodes, graph):\n",
    "    clr = ['r', 'g', 'b', 'y', 'm']\n",
    "\n",
    "    for i, node in enumerate(most_influential_nodes):\n",
    "        y_true = graph.nodes.get(node[0])['pred']\n",
    "        y_pred = graph.nodes.get(node[0])['polyfit_prediction']\n",
    "        ts = list(map(lambda x: x[0], y_true))\n",
    "        plt.plot(ts, np.cumsum(list(map(lambda x: x[1], y_true))), 'o', color=clr[i], markersize=2, label=f\"True Node: {i}\")\n",
    "        plt.plot(ts, y_pred, '--', color=clr[i], label=f\"Predicted Node {i}\")\n",
    "\n",
    "    # save the plot\n",
    "    plot_name = 's.png'\n",
    "    plt.xlabel('Timestamps')\n",
    "    plt.ylabel('Influence')\n",
    "    plt.legend(loc=\"upper left\")    \n",
    "    plt.savefig(plot_name)\n",
    "    plt.show()\n",
    "\n",
    "def plot_2(most_influential_nodes, graph):\n",
    "    clr = ['r', 'g', 'b', 'y', 'm']\n",
    "    predicted_data = []\n",
    "    true_data = []\n",
    "    ts = []\n",
    "    for i, node in enumerate(most_influential_nodes):\n",
    "        ts.append(list(map(lambda x: x[0], graph.nodes.get(node[0])['pred'])))\n",
    "        true_data.append(np.cumsum(list(map(lambda x: x[1], graph.nodes.get(node[0])['pred']))))\n",
    "        predicted_data.append(graph.nodes.get(node[0])['polyfit_prediction'])\n",
    "\n",
    "    # Set the seaborn style\n",
    "    sns.set(style='whitegrid')\n",
    "\n",
    "    # Set up the figure and subplots\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 20))\n",
    "    fig.tight_layout(pad=6.0)\n",
    "\n",
    "    # Set a custom color palette\n",
    "    palette = sns.color_palette(\"husl\", 2)\n",
    "\n",
    "    for i, (predicted, true) in enumerate(zip(predicted_data, true_data)):\n",
    "        sns.lineplot(x=ts[i], y=predicted, label='Predicted', linewidth=2, linestyle='-', marker='o', markersize=8, color=palette[0], ax=axes[i])\n",
    "        sns.lineplot(x=ts[i], y=true, label='True', linewidth=2, linestyle='-', marker='x', markersize=8, color=palette[1], ax=axes[i])\n",
    "\n",
    "        axes[i].set_title(f'Most influencing node #{i+1}', fontsize=16, fontweight='bold', pad=15)\n",
    "        axes[i].legend(fontsize=12, loc='upper right')\n",
    "        axes[i].set_xlabel('Timestamps', fontsize=14)\n",
    "        axes[i].set_ylabel('Influence', fontsize=14)\n",
    "        axes[i].tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    # Remove the top and right spines for a cleaner look\n",
    "    sns.despine()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T17:03:09.880721Z",
     "start_time": "2023-04-15T17:03:09.880497Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate(dataset, delta_t, phi, beta, geterror=False):\n",
    "    print(f'Running simulation for dataset {dataset} with delta_t {delta_t} and phi {phi} and beta {beta}')\n",
    "    flag = True\n",
    "\n",
    "    while flag:\n",
    "        flag = False \n",
    "        all_data, train_data, predict_data, train_lines, total_edges, start, mid, end, new_points = get_data(dataset, delta_t, phi)\n",
    "        src, dst, ts, graph = make_graph(all_data)\n",
    "        most_influential_nodes = find_influence_in_graph(graph, all_data, beta) # 5 most influential nodes [(node id, influence)]\n",
    "        \n",
    "        for node in most_influential_nodes:\n",
    "            thisnode = graph.nodes.get(node[0])\n",
    "            if thisnode is not {}:\n",
    "                thisnode['influences'], thisnode['pred'] = fill_holes_timestamps(train_data[0][2], \\\n",
    "                                                                                 predict_data[0][2], \\\n",
    "                                                                                 predict_data[-1][2], \\\n",
    "                                                                                 thisnode['influences'])\n",
    "                if len(thisnode['pred']) > 3:\n",
    "                    thisnode['polyfit_prediction'] = polyfit(thisnode['influences'], new_points=new_points, dimension=1)\n",
    "\n",
    "        \n",
    "        for node in most_influential_nodes:\n",
    "            if not bool(graph.nodes.get(node[0])['pred']):\n",
    "                flag = True\n",
    "\n",
    "    if geterror:\n",
    "        return np.average((np.array(thisnode['pred'])[:, 0] - thisnode['polyfit_prediction']) ** 2)\n",
    "    \n",
    "    plot_1(most_influential_nodes, graph)\n",
    "    plot_2(most_influential_nodes, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T17:03:12.104290Z",
     "start_time": "2023-04-15T17:03:09.880548Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8f6baf13f846ba9bcd78e9047bbc96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='infectious', description='dataset'), FloatSlider(value=0.1, description='del…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_dropdown = widgets.Dropdown(options=['infectious', 'ht09_contact', 'SFHH', 'tij_InVS15'], description='Dataset', disabled=False, continuous_update=True)\n",
    "phi_slider = widgets.FloatSlider(min=0.1, max=1, step=0.1, value=0.1, continuous_update=True)\n",
    "delta_t_slider = widgets.FloatSlider(min=0.1, max=1, step=0.1, value=0.1, continuous_update=True)\n",
    "beta_slider = widgets.FloatSlider(min=0.1, max=1, step=0.1, value=0.75, continuous_update=True)\n",
    "\n",
    "w = widgets.interactive(simulate, dataset=dataset_dropdown.value, delta_t=delta_t_slider, phi=phi_slider, beta=beta_slider)\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T17:03:12.152557Z",
     "start_time": "2023-04-15T17:03:12.098041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 937 till 2493 to predict till 2666\n",
      "Starting at [ 10  14 135] till [ 71  73 253] to predict till [ 34  35 264]\n",
      "most influentual nodes: [(42, 102), (71, 78), (43, 73)]\n",
      "0.1\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 1566 till 3122 to predict till 3295\n",
      "Starting at [ 59  62 190] till [ 41  92 292] to predict till [ 37  38 301]\n",
      "most influentual nodes: [(71, 124), (70, 101), (73, 100)]\n",
      "0.15000000000000002\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 11980 till 13536 to predict till 13709\n",
      "Starting at [276 278 769] till [281 299 862] to predict till [263 266 873]\n",
      "most influentual nodes: [(259, 89), (297, 89), (284, 88)]\n",
      "0.2\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 5283 till 6839 to predict till 7012\n",
      "Starting at [ 57  58 413] till [163 193 495] to predict till [151 159 506]\n",
      "most influentual nodes: [(142, 74), (160, 64), (113, 59)]\n",
      "0.25\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 6876 till 8432 to predict till 8605\n",
      "Starting at [128 189 498] till [215 218 560] to predict till [239 235 567]\n",
      "most influentual nodes: [(175, 83), (215, 66), (214, 65)]\n",
      "0.30000000000000004\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 10175 till 11731 to predict till 11904\n",
      "Starting at [197 206 649] till [284 297 758] to predict till [262 264 766]\n",
      "most influentual nodes: [(193, 107), (191, 104), (192, 99)]\n",
      "0.35000000000000003\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 3737 till 5293 to predict till 5466\n",
      "Starting at [ 73 115 328] till [126 131 414] to predict till [121 130 423]\n",
      "most influentual nodes: [(125, 86), (122, 83), (132, 77)]\n",
      "0.4\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 4148 till 5704 to predict till 5877\n",
      "Starting at [ 86  87 352] till [139 141 436] to predict till [140 141 444]\n",
      "most influentual nodes: [(122, 94), (125, 87), (123, 77)]\n",
      "0.45\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 6514 till 8070 to predict till 8243\n",
      "Starting at [176 177 475] till [176 178 546] to predict till [203 199 553]\n",
      "most influentual nodes: [(113, 65), (175, 64), (176, 60)]\n",
      "0.5\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 729 till 2285 to predict till 2458\n",
      "Starting at [ 33  36 116] till [ 78  81 238] to predict till [ 70  73 250]\n",
      "most influentual nodes: [(37, 97), (42, 91), (39, 90)]\n",
      "0.55\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 2490 till 4046 to predict till 4219\n",
      "Starting at [ 50  51 253] till [ 96 104 345] to predict till [ 96 108 357]\n",
      "most influentual nodes: [(71, 109), (70, 91), (73, 61)]\n",
      "0.6000000000000001\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 15356 till 16912 to predict till 17085\n",
      "Starting at [ 272  356 1051] till [ 402  403 1294] to predict till [ 386  387 1334]\n",
      "most influentual nodes: [(386, 138), (387, 121), (345, 115)]\n",
      "0.65\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 12751 till 14307 to predict till 14480\n",
      "Starting at [257 258 810] till [345 347 973] to predict till [357 358 985]\n",
      "most influentual nodes: [(297, 99), (296, 81), (266, 72)]\n",
      "0.7000000000000001\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 13867 till 15423 to predict till 15596\n",
      "Starting at [320 322 882] till [ 353  352 1056] to predict till [ 332  340 1072]\n",
      "most influentual nodes: [(332, 119), (330, 116), (331, 113)]\n",
      "0.75\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 3626 till 5182 to predict till 5355\n",
      "Starting at [ 56  57 323] till [120 126 408] to predict till [152 153 417]\n",
      "most influentual nodes: [(122, 90), (132, 78), (125, 72)]\n",
      "0.8\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 10415 till 11971 to predict till 12144\n",
      "Starting at [227 232 660] till [291 293 768] to predict till [195 216 779]\n",
      "most influentual nodes: [(191, 93), (193, 91), (192, 83)]\n",
      "0.8500000000000001\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 6202 till 7758 to predict till 7931\n",
      "Starting at [167 168 458] till [156 204 535] to predict till [176 177 540]\n",
      "most influentual nodes: [(176, 76), (182, 67), (113, 62)]\n",
      "0.9\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 9105 till 10661 to predict till 10834\n",
      "Starting at [244 242 590] till [191 193 689] to predict till [268 271 707]\n",
      "most influentual nodes: [(192, 77), (200, 76), (191, 71)]\n",
      "0.9500000000000001\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 7301 till 8857 to predict till 9030\n",
      "Starting at [214 215 519] till [231 232 580] to predict till [117 226 588]\n",
      "most influentual nodes: [(215, 78), (218, 77), (214, 69)]\n",
      "1.0\n",
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 0.7\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 8482 till 10038 to predict till 10211\n",
      "Starting at [115 182 564] till [163 192 641] to predict till [192 194 651]\n",
      "most influentual nodes: [(219, 96), (196, 82), (200, 75)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.05, 65144.636615211326),\n",
       " (0.1, 85118.72792477926),\n",
       " (0.15000000000000002, 741903.2122022449),\n",
       " (0.2, 243247.23772064177),\n",
       " (0.25, 309846.0396262654),\n",
       " (0.30000000000000004, 576484.0854597888),\n",
       " (0.35000000000000003, 170678.74397167022),\n",
       " (0.4, 188355.55346618724),\n",
       " (0.45, 296068.97372302396),\n",
       " (0.5, 57742.35528721695),\n",
       " (0.55, 119877.08446361321),\n",
       " (0.6000000000000001, 1729645.0196376),\n",
       " (0.65, 952513.9981632548),\n",
       " (0.7000000000000001, 1102252.2571861257),\n",
       " (0.75, 167985.04652063953),\n",
       " (0.8, 593706.9002209582),\n",
       " (0.8500000000000001, 286650.0335923238),\n",
       " (0.9, 469781.8993578274),\n",
       " (0.9500000000000001, 335364.71442749916),\n",
       " (1.0, 406397.62391281634)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas = []\n",
    "for beta in range(1, 21):\n",
    "    beta = beta * 0.05\n",
    "    print(beta)\n",
    "    betas.append((beta, simulate(dataset=dataset_dropdown.value, delta_t=delta_t_slider.value, \\\n",
    "             phi=phi_slider.value, beta=beta_slider.value, geterror=True)))\n",
    "betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e35c14a8caca76198b34fb255762891fb38351c0ec24f1fa8546521f6a1a9596"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
