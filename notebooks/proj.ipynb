{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "dataset = 'tij_InVS15'\n",
    "\n",
    "beta = 0.2\n",
    "phi = 0.2\n",
    "\n",
    "# TODO: Should be chosen based on the dataset\n",
    "delta_t = 1000\n",
    "\n",
    "# Same here, probably?\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a DataFrame.\n",
    "data: pd.DataFrame = pd.read_csv(f\"../data/{dataset}.txt\", sep=\"\\t\", header=None, names=[\"source\", \"target\", \"ts\"])\n",
    "\n",
    "# Get unique timestamps.\n",
    "timestamps = data[\"ts\"].unique()\n",
    "\n",
    "# Remap timestamps to remove gaps.\n",
    "# This makes the timestamps equvivalent to range(len(timestamps)),\n",
    "# which allows us to do `for t in range(t0, round(phi*delta_t) + 1)` effortlessly.\n",
    "ts_mapping = {}\n",
    "r = range(len(timestamps))\n",
    "for i, t in enumerate(timestamps):\n",
    "    if t != r[i]:\n",
    "        ts_mapping[t] = i\n",
    "\n",
    "data[\"ts\"].replace(to_replace=ts_mapping, inplace=True)\n",
    "\n",
    "timestamps = data[\"ts\"].unique()\n",
    "\n",
    "# Construct graph from DataFrame, specifying MultiGraph and timestamp as edge attribute.\n",
    "graph: nx.MultiGraph = nx.from_pandas_edgelist(data, create_using=nx.MultiGraph, edge_attr=\"ts\", edge_key=\"ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compute_influences\n",
    "\n",
    "rng = numpy.random.default_rng(12345)\n",
    "\n",
    "node_influences = compute_influences(graph, data, delta_t, beta, rng)\n",
    "\n",
    "print(\"Nodes & their influence in descending order: \")\n",
    "for (node, influence) in sorted(node_influences.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"#{node}: {influence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import slice_dataset\n",
    "\n",
    "t0 = 1000 # TODO: Pick the right t0.\n",
    "\n",
    "sub_data = slice_dataset(data, t0, delta_t, phi)\n",
    "\n",
    "sub_graph: nx.MultiGraph = nx.from_pandas_edgelist(sub_data, create_using=nx.MultiGraph, edge_attr=\"ts\", edge_key=\"ts\")\n",
    "\n",
    "aggregated_sub_data = sub_data.drop(columns=[\"ts\"])\n",
    "aggregated_sub_graph: nx.Graph = nx.from_pandas_edgelist(aggregated_sub_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Graph\n",
    "\n",
    "src, dst, ts = [], [], []\n",
    "G = nx.MultiGraph()\n",
    "\n",
    "with open(f'../data/{dataset}.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = list(map(int, line.strip().split('\\t')))\n",
    "        src.append(line[0])\n",
    "        dst.append(line[1])\n",
    "        ts.append(line[2])\n",
    "\n",
    "        G.add_node(line[0], is_infected=False, infected_at=math.inf)\n",
    "        G.add_node(line[1], is_infected=False, infected_at=math.inf)\n",
    "\n",
    "        G.add_edge(line[0], line[1], key=line[2], timestamp=line[2])\n",
    "\n",
    "src, dst, ts = np.array(src), np.array(dst), np.array(ts) # ts is given sorted\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([src, dst, ts])\n",
    "a = a.T\n",
    "# np.any(np.all(np.unique(a, axis=1) == a, axis=1))\n",
    "u, c = np.unique(a, return_counts=True, axis=0)\n",
    "u[c > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate node influences\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "node_influences = []\n",
    "\n",
    "# for each node in the graph\n",
    "for node in G.nodes:\n",
    "    if not np.any(src == node):\n",
    "        node_influences.append(0)\n",
    "        continue\n",
    "    t_0 = np.min(ts[src == node])\n",
    "\n",
    "    G.nodes[node]['is_infected'] = True\n",
    "    G.nodes[node]['infected_at'] = t_0\n",
    "\n",
    "    # for every timestep in (t_0, t_0 + Delta_T)\n",
    "    for t in range(t_0 + 1, t_0 + delta_t):\n",
    "        # if off bounds, continue\n",
    "        if t not in ts:\n",
    "            continue\n",
    "        # for every node in the graph\n",
    "        for n, nbrs in G.adjacency():\n",
    "            # if the node is infected\n",
    "            if G.nodes[n]['is_infected']:\n",
    "                # get its neighbors at that timestamp\n",
    "                for nbr, eattr in nbrs.items():\n",
    "                    if t in eattr.keys() and G.nodes[n]['infected_at'] < t:\n",
    "                        # infect them with probability b\n",
    "                        if rng.uniform() <= beta:\n",
    "                            # print(f\"Marking {nbr} as infected at {t} since {n} was infected at {G.nodes[n]['infected_at']}\")\n",
    "                            G.nodes[nbr]['is_infected'] = True\n",
    "                            G.nodes[nbr]['infected_at'] = t\n",
    "\n",
    "    # node influence\n",
    "    node_influences.append(len([n for n in G.nodes if G.nodes[n]['is_infected']]) / len(G.nodes))\n",
    "\n",
    "    # reset graph\n",
    "    for n in G.nodes:\n",
    "        G.nodes[n]['is_infected'] = False\n",
    "\n",
    "node_influences = np.array(node_influences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct subgraphs and calculate subgraph-related metrics\n",
    "\n",
    "####### labels #######\n",
    "labels = [\n",
    "    'Number of union of contacts in [t_0, t_0 + phi*Delta_T]',\n",
    "    'Closeness Centrality',\n",
    "    # 'Wiener Indices',\n",
    "    'Betweenness Centrality',\n",
    "    # 'Subgraph Centrality',\n",
    "    'VoteRank',\n",
    "    'Degree',\n",
    "    'Degree Assortativity Coefficient',\n",
    "    'Harmonic Centrality',\n",
    "    'Local Reaching Centrality',\n",
    "    # 'Eigenvector Centrality'\n",
    "]\n",
    "\n",
    "metrics = [[] for _ in labels]\n",
    "for node in G.nodes:\n",
    "    if not np.any(src == node) or not np.any(dst == node):\n",
    "        for i in range(len(metrics)):\n",
    "            metrics[i].append(0)\n",
    "        continue\n",
    "    # get the index of the first time that node appears\n",
    "    start_src_idx = np.min(np.arange(len(src))[src == node])\n",
    "    start_dst_idx = np.min(np.arange(len(src))[dst == node])\n",
    "    start_idx = min(start_src_idx, start_dst_idx)\n",
    "    # t_0 = ts[start_idx]\n",
    "\n",
    "    subgraph = nx.MultiGraph()  # subgraph in (t_0, t_0 + Delta_T)\n",
    "    i = start_idx\n",
    "    j = ts[i]\n",
    "    while i < len(src) and ts[i] < j + int(phi * delta_t):\n",
    "        subgraph.add_edge(src[i], dst[i], key=ts[i])\n",
    "        i += 1\n",
    "\n",
    "    ####### metrics #######\n",
    "    metrics[0].append(len(nx.single_source_shortest_path(subgraph, node, cutoff=k)))\n",
    "    metrics[1].append(nx.closeness_centrality(subgraph, node))\n",
    "    # metrics[2].append(nx.wiener_index(subgraph))\n",
    "    metrics[2].append(nx.betweenness_centrality(subgraph)[node])\n",
    "    # metrics[3].append(nx.subgraph_centrality(subgraph)[node])\n",
    "    metrics[3].append(np.argmax(nx.voterank(subgraph)))\n",
    "    metrics[4].append(subgraph.degree[node])\n",
    "    metrics[5].append(nx.degree_assortativity_coefficient(subgraph))\n",
    "    metrics[6].append(nx.harmonic_centrality(subgraph)[node])\n",
    "    metrics[7].append(nx.local_reaching_centrality(subgraph, node))\n",
    "    # metrics[10].append(nx.eigenvector_centrality(subgraph)[node])\n",
    "\n",
    "for i, m in enumerate(metrics):\n",
    "    m = np.array(m)\n",
    "    m = np.abs(m)\n",
    "    ma = np.max(m[m != np.inf])\n",
    "    m = m / ma # scale it to 0-1\n",
    "    m[m == np.inf] = 1\n",
    "    metrics[i] = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSEs\n",
    "\n",
    "y = node_influences\n",
    "mse = []\n",
    "for m in metrics:\n",
    "    mse.append((np.sum(m - y)**2) / len(y))\n",
    "list(zip(labels, mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "# sort to compare cumulatively\n",
    "sort = True\n",
    "\n",
    "x = np.arange(len(node_influences))\n",
    "\n",
    "plot_metrics = metrics.copy()\n",
    "\n",
    "sort_idxs = np.argsort(node_influences)\n",
    "y = node_influences[sort_idxs] if sort else node_influences\n",
    "for i, m in enumerate(metrics):\n",
    "    plot_metrics[i] = m[sort_idxs] if sort else m\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(20, 12))\n",
    "ax1.plot(x, y, '.-', label='Node Influences')\n",
    "for i, m in enumerate(plot_metrics[:4]):\n",
    "    ax1.plot(x, m, label=labels[i])\n",
    "plt.title(dataset)\n",
    "plt.legend()\n",
    "plt.savefig(f'{dataset}{\"-cumulative\" if sort else \"\"}.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions\n",
    "\n",
    "Useful metrics that seem to model closely to the node influences, apart from Num. of union of contacts:\n",
    "\n",
    "|dataset      | n_nodes | n_edges | beta |Delta_T|duration|k    |phi  |best metrics|\n",
    "| ----------- |-------- | ------- | ---- | ----- | ------ | --- | --- | ---------- |\n",
    "| ht09        |  113    |  20818  | 0.8  | 1000  | 27s    | 5   | 0.5 | Closeness Centrality, Degree Assortativity Coefficient     |\n",
    "| infectious  |     410 |  17298  | 0.6  | 500   | 67s    | 5   | 0.5 | Eigenvalue Centrality does not converge, so removed. Betweeness/Subgraph centrality. |\n",
    "| SFHH        |     403 | 70261   | 0.6  | 500   |  180s  | 5   | 0.5 | Eigen. not converging, removed. Num of union of contacts, Degree, Harmonic/Local-Reaching Centralities. |    \n",
    "| tij_lnVS15  |   217   | 78249   | 0.6  | 500   |  21s   | 5   | 0.5 | Eigen. again not conv., removed. Betweeness/Subgraph centralities |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = np.polyfit(y, np.array([metrics[0], metrics[1]]).T, 1) #The use of 1 signifies a linear fit.\n",
    "# line = np.poly1d(fit)\n",
    "# len(np.array([metrics[0], metrics[1]]).T)\n",
    "fit\n",
    "np.poly(fit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e35c14a8caca76198b34fb255762891fb38351c0ec24f1fa8546521f6a1a9596"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
