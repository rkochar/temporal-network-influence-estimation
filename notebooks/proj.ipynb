{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from ipywidgets import widgets\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# dataset = 'infectious'\n",
    "# beta = 1\n",
    "# Delta_T = 0.1 # Fraction of dataset to select (this is then divided by phi)\n",
    "# k = 10\n",
    "# phi = 0.1\n",
    "# lines = 0\n",
    "# runs = 3\n",
    "\n",
    "def get_data(dataset, Delta_T, phi):\n",
    "    lines = 0\n",
    "    if dataset == \"infectious\":\n",
    "        lines = 17298\n",
    "    elif dataset == \"ht09_contact\":\n",
    "        lines = 20818\n",
    "    elif dataset == \"SFHH\":\n",
    "        lines = 70261\n",
    "    elif dataset == \"tij\":\n",
    "        lines = 78249\n",
    "\n",
    "    total_edges = int(lines * Delta_T)\n",
    "    random_start_point = np.random.uniform(0, float(1 - Delta_T))\n",
    "    start = int(lines * random_start_point)\n",
    "\n",
    "    train_lines = int(total_edges * float(1 - phi))\n",
    "    mid = start + train_lines\n",
    "    end = start + total_edges\n",
    "\n",
    "    print(f'Total lines: {total_edges}, train lines {train_lines}')\n",
    "    print(f'Starting at {start} till {mid} to predict till {end}')\n",
    "\n",
    "    path_to_file = f'../data/{dataset}.txt'\n",
    "\n",
    "    all_data = np.loadtxt(path_to_file, delimiter='\\t', dtype=int)[start: end]\n",
    "    train_data = all_data[:train_lines]\n",
    "    predict_data = all_data[mid:]\n",
    "\n",
    "    return all_data, train_data, predict_data, train_lines, total_edges, start, mid, end\n",
    "\n",
    "# Check all edges are unique\n",
    "\n",
    "# a = np.array([src, dst, ts])\n",
    "# a = a.T\n",
    "# # np.any(np.all(np.unique(a, axis=1) == a, axis=1))\n",
    "# u, c = np.unique(a, return_counts=True, axis=0)\n",
    "# u[c > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Graph\n",
    "def make_graph(all_data):\n",
    "    src, dst, ts = [], [], []\n",
    "    graph = nx.MultiGraph()\n",
    "\n",
    "    for line in all_data:\n",
    "        src.append(line[0])\n",
    "        dst.append(line[1])\n",
    "        ts.append(line[2])\n",
    "\n",
    "        if graph.nodes.get(line[0]) is None:\n",
    "            graph.add_node(line[0], is_infected=False, infected_at=math.inf, influence=-1, influences=None, pred=None, polyfit_prediction=None)\n",
    "        if graph.nodes.get(line[1]) is None:\n",
    "            graph.add_node(line[1], is_infected=False, infected_at=math.inf, influence=-1, influences=None, pred=None, polyfit_prediction=None)\n",
    "\n",
    "        graph.add_edge(line[0], line[1], key=line[2], timestamp=line[2])\n",
    "\n",
    "    src, dst, ts = np.array(src), np.array(dst), np.array(ts) # ts is given sorted\n",
    "#     print(graph.nodes.get(src[0]))\n",
    "\n",
    "    return src, dst, ts, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_edge(edge, g, inf, ins, beta):\n",
    "    # TODO: use beta\n",
    "    if g.nodes.get(edge[0])['is_infected'] and edge[2] > g.nodes.get(edge[0])['infected_at'] and not g.nodes.get(edge[1])['is_infected']:\n",
    "        inf += 1\n",
    "        g.nodes.get(edge[1])['in_infected'] = True\n",
    "        g.nodes.get(edge[1])['infected_at'] = edge[2]\n",
    "        ins[str(edge[2])] = ins.get(str(edge[2]), 0) + 1\n",
    "    \n",
    "    return inf, ins, g\n",
    "\n",
    "def find_influence_of_node(edges, node, at, train_lines, beta):\n",
    "    g = nx.Graph()\n",
    "    influence, influences, pred = 0, {}, {}\n",
    "    \n",
    "    for edge in edges:\n",
    "        if g.nodes.get(edge[0]) is None:\n",
    "            g.add_node(edge[0], is_infected=False, infected_at=math.inf)\n",
    "        if g.nodes.get(edge[1]) is None:\n",
    "            g.add_node(edge[1], is_infected=False, infected_at=math.inf)\n",
    "\n",
    "    g.nodes.get(node)['is_infected'] = True\n",
    "    g.nodes.get(node)['infected_at'] = at\n",
    "\n",
    "    # TODO: Check influence. I think it's getting unset somewhere because of how Python handles pointers.\n",
    "    for i, edge in enumerate(edges):\n",
    "        if i <= train_lines:\n",
    "            influence, influences, g = check_edge(edge, g, influence, influences, beta)\n",
    "            new_edge = (edge[1], edge[0], edge[2])\n",
    "            influence, influences, g = check_edge(new_edge, g, influence, influences, beta)\n",
    "        else:\n",
    "            influence, pred, g = check_edge(edge, g, influence, pred, beta)\n",
    "            new_edge = (edge[1], edge[0], edge[2])\n",
    "            influence, pred, g = check_edge(new_edge, g, influence, pred, beta)\n",
    "                \n",
    "    return influence, influences, pred\n",
    "\n",
    "# TODO: Fix this\n",
    "def find_influence_in_graph(graph, all_data, train_lines, beta):\n",
    "    for i, edge in enumerate(all_data):\n",
    "        for e in [edge[0], edge[1]]:\n",
    "            if graph.nodes.get(e)['influence'] == -1:\n",
    "                z, zz, zzz = find_influence_of_node(all_data, e, edge[2], train_lines, beta)\n",
    "                graph.nodes.get(e)['influence'] = z\n",
    "                graph.nodes.get(e)['influences'] = zz\n",
    "                if zzz is not {}:\n",
    "                    graph.nodes.get(e)['pred'] = zzz # time series of influence. [(0, 0), (1, 5), (2, 10), (4, 12)]\n",
    "\n",
    "    most_influential_nodes = sorted(list(graph.nodes.data(\"influence\")), key=lambda x: x[1], reverse=True)[:5]\n",
    "    print(f\"most influentual nodes: {most_influential_nodes}\")\n",
    "    return most_influential_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyfit(data, new_points, dimension):\n",
    "    print(f\"new_points: {new_points}\")\n",
    "    k, v = list(map(lambda x: int(x), data.keys())), list(data.values())\n",
    "    print(f\"k: {len(k)}\")\n",
    "    print(f\"v: {len(v)}\")\n",
    "    \n",
    "    fit = np.polyfit(k, v, 1) #The use of 1 signifies a linear fit.\n",
    "\n",
    "    line = np.poly1d(fit)\n",
    "    new_points = np.arange(new_points) + (k[-1] + 1)\n",
    "    new_p = line(new_points)\n",
    "    print(f'New points predicted are: {len(new_p)}')\n",
    "\n",
    "    return new_p\n",
    "#     return np.cumsum(line(new_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(most_influential_nodes, graph):\n",
    "    clr = ['r', 'g', 'b', 'y', 'm']\n",
    "\n",
    "    for i, node in enumerate(most_influential_nodes):\n",
    "        plt.plot(list(graph.nodes.get(node[0])['pred'].keys()), list(graph.nodes.get(node[0])['pred'].values()), 'o', color=clr[i])\n",
    "        #plt.plot(list(graph.nodes.get(node[0])['polyfit_prediction'].keys()), list(graph.nodes.get(node[0])['polyfit_prediction'].values()), 'o')\n",
    "#         print(graph.nodes.get(node[0])['pred'])\n",
    "    # save the plot\n",
    "    plot_name = 's.png'\n",
    "    plt.savefig(plot_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(dataset, delta_t, phi, beta):\n",
    "    print(f'Running simulation for dataset {dataset} with delta_t {delta_t} and phi {phi} and beta {beta}')\n",
    "    flag = True\n",
    "\n",
    "    while flag:\n",
    "        flag = False\n",
    "        all_data, train_data, predict_data, train_lines, total_edges, start, mid, end = get_data(dataset, delta_t, phi)\n",
    "        src, dst, ts, graph = make_graph(all_data)\n",
    "        most_influential_nodes = find_influence_in_graph(graph, all_data, train_lines, beta) # 5 most influential nodes [(node id, influence)]\n",
    "        \n",
    "        print(f\"Predict for mid: {mid} till end {end}\")\n",
    "        for node in most_influential_nodes:\n",
    "            thisnode = graph.nodes.get(node[0])\n",
    "            if thisnode is not {} and len(thisnode['pred']) > 3:\n",
    "                print(f\"node: {node[0]}\")\n",
    "                print(f\"preds: {thisnode['pred']}\\n\\n\")\n",
    "                for i in range(mid, end + 1):\n",
    "                    if thisnode['pred'].get(i) is None:\n",
    "                        thisnode['pred'][i] = 0 # thisnode['influences'][i - 1]  # Can do cumsum here and make it more efficient\n",
    "                thisnode['polyfit_prediction'] = polyfit(thisnode['influences'], new_points=end - mid, dimension=1)\n",
    "\n",
    "        \n",
    "        for node in most_influential_nodes:\n",
    "            if not bool(graph.nodes.get(node[0])['pred']):\n",
    "                flag = True\n",
    "\n",
    "    # Find all the unique keys in most_influential_nodes\n",
    "#     keys_pred = set()\n",
    "#     keys_polyfit = set()\n",
    "#     for node in most_influential_nodes:\n",
    "#         keys_pred.update(graph.nodes.get(node[0])['pred'].keys())\n",
    "#         keys_polyfit.update(graph.nodes.get(node[0])['polyfit_prediction'].keys())\n",
    "    \n",
    "    # if a node is missing a key, add it and set it to 0\n",
    "    for node in most_influential_nodes:\n",
    "        for key in keys_pred:\n",
    "            if key not in graph.nodes.get(node[0])['pred']:\n",
    "                graph.nodes.get(node[0])['pred'][key] = 0\n",
    "        for key in keys_polyfit:\n",
    "            if key not in graph.nodes.get(node[0])['polyfit_prediction']:\n",
    "                graph.nodes.get(node[0])['polyfit_prediction'][key] = 0\n",
    "                \n",
    "    # for each node sort keys\n",
    "    for node in most_influential_nodes:\n",
    "        graph.nodes.get(node[0])['pred'] = dict(sorted(graph.nodes.get(node[0])['pred'].items(), key=lambda item: item[0]))\n",
    "        graph.nodes.get(node[0])['polyfit_prediction'] = dict(sorted(graph.nodes.get(node[0])['polyfit_prediction'].items(), key=lambda item: item[0]))\n",
    "\n",
    "    plot(most_influential_nodes, graph)\n",
    "    all_data, train_data, predict_data, train_lines, total_edges, start, mid, end = get_data(dataset, delta_t, phi)\n",
    "    src, dst, ts, graph = make_graph(all_data)\n",
    "    most_influential_nodes = find_influence_in_graph(graph, all_data, train_lines, beta) # 5 most influential nodes [(node id, influence)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running simulation for dataset infectious with delta_t 0.1 and phi 0.1 and beta 1\n",
      "Total lines: 1729, train lines 1556\n",
      "Starting at 11164 till 12720 to predict till 12893\n",
      "most influentual nodes: [(284, 116), (296, 113), (291, 110), (287, 109), (289, 109)]\n",
      "Predict for mid: 12720 till end 12893\n",
      "node: 284\n",
      "preds: {'808': 1, '809': 2, '810': 1, '813': 3, '814': 3, '816': 2, '817': 2, '818': 2, '819': 2}\n",
      "\n",
      "\n",
      "new_points: 173\n",
      "k: 50\n",
      "v: 50\n",
      "New points predicted are: 173\n",
      "node: 296\n",
      "preds: {'808': 2, '809': 1, '811': 3, '812': 3, '813': 4, '814': 4, '815': 2, '816': 1, '818': 2, '819': 1}\n",
      "\n",
      "\n",
      "new_points: 173\n",
      "k: 50\n",
      "v: 50\n",
      "New points predicted are: 173\n",
      "node: 291\n",
      "preds: {'808': 2, '809': 1, '810': 1, '811': 3, '812': 1, '813': 2, '814': 3, '815': 1, '817': 2, '818': 2, '819': 1}\n",
      "\n",
      "\n",
      "new_points: 173\n",
      "k: 53\n",
      "v: 53\n",
      "New points predicted are: 173\n",
      "node: 287\n",
      "preds: {'809': 2, '812': 1, '813': 3, '816': 2, '817': 2, '818': 1, '819': 1, '820': 1}\n",
      "\n",
      "\n",
      "new_points: 173\n",
      "k: 48\n",
      "v: 48\n",
      "New points predicted are: 173\n",
      "node: 289\n",
      "preds: {'808': 2, '809': 1, '810': 2, '811': 1, '812': 1, '813': 2, '814': 3, '816': 1, '818': 1, '819': 3}\n",
      "\n",
      "\n",
      "new_points: 173\n",
      "k: 48\n",
      "v: 48\n",
      "New points predicted are: 173\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'keys_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minfectious\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 36\u001b[0m, in \u001b[0;36msimulate\u001b[0;34m(dataset, delta_t, phi, beta)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Find all the unique keys in most_influential_nodes\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#     keys_pred = set()\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     keys_polyfit = set()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# if a node is missing a key, add it and set it to 0\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m most_influential_nodes:\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mkeys_pred\u001b[49m:\n\u001b[1;32m     37\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mget(node[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     38\u001b[0m                 graph\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mget(node[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m][key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keys_pred' is not defined"
     ]
    }
   ],
   "source": [
    "simulate(\"infectious\", 0.1, 0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dropdown = widgets.Dropdown(options=['infectious', 'ht09_contact', 'SFHH', 'tij_InVS15'], description='Dataset', disabled=False, continuous_update=True)\n",
    "phi_slider = widgets.FloatSlider(min=0.1, max=0.9, step=0.01, value=0.1, continuous_update=True)\n",
    "delta_t_slider = widgets.FloatSlider(min=0, max=1, step=0.01, value=0.1, continuous_update=True)\n",
    "beta_slider = widgets.FloatSlider(min=0, max=1, step=0.01, value=0.75, continuous_update=True)\n",
    "\n",
    "w = widgets.interactive(simulate, dataset=dataset_dropdown.value, delta_t=delta_t_slider.value, phi=phi_slider.value, beta=beta_slider.value)\n",
    "display(w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e35c14a8caca76198b34fb255762891fb38351c0ec24f1fa8546521f6a1a9596"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
